{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RP-Crowd-3 Analysis\n",
    "In this notebook we analyse the RP-Crowd-3 dataset and train our baseline classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install autosklearn\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "##skopt\n",
    "from skopt import BayesSearchCV\n",
    "from skopt import dump\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "\n",
    "##sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "## automl\n",
    "#import autosklearn.classification\n",
    "import six.moves.cPickle as pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first we load the dataset with the predefined folds\n",
    "\n",
    "experiment = \"abusive_min_3\"\n",
    "out = \"experiments\"\n",
    "## load data\n",
    "df = pd.read_csv(\"../../../Dataset/Text-Data/RP-Crowd-3-folds.csv\")\n",
    "\n",
    "## load labels and cast to int\n",
    "y_dat = df[\"label\"].values\n",
    "y_dat = y_dat.astype(int)\n",
    "\n",
    "## test data filter\n",
    "filter_q =  df['ten_folds'] < 8 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessingTransformer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        documents = []\n",
    "        nlp = spacy.load(\"de_core_news_lg\")\n",
    "        for sen in tqdm(range(0, len(X))):\n",
    "            # Remove all the special characters\n",
    "            document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "\n",
    "            # Remove numbers\n",
    "            document = re.sub(r'[0-9]', ' ', document)\n",
    "\n",
    "            # remove all single characters\n",
    "            document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "            # Remove single characters from the start\n",
    "            document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "            # Substituting multiple spaces with single space\n",
    "            document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "            # Removing prefixed 'b'\n",
    "            document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "            # Converting to Lowercase\n",
    "            document = document.lower()\n",
    "\n",
    "            # Lemmatization\n",
    "            document = nlp(document)\n",
    " \n",
    "            document = [word.lemma_ for word in document]\n",
    "            document = ' '.join(document)\n",
    "        \n",
    "            documents.append(document)\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download de_core_news_lg\n",
    "nltk.download(\"stopwords\")\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(stop_words=german_stop_words, min_df = 5, ngram_range=(1,2), max_features=3224)\n",
    "preprocessor = TextPreprocessingTransformer()\n",
    "\n",
    "\n",
    "preprocessed = preprocessor.transform(np.array(df[\"text\"]))\n",
    "\n",
    "tfidf_dat = tf.fit_transform(preprocessed).toarray()\n",
    "tfidf_dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify test data\n",
    "\n",
    "test = tfidf_dat[-filter_q]\n",
    "test_y = y_dat[-filter_q]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF-Evaluation\n",
    "Now we start building models based on TF-IDF representation. We build the following models:\n",
    "1. Naive Bayes\n",
    "2. Logistic Regression\n",
    "3. Gradient Boosted Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    params_NB = {\n",
    "        \"alpha\":(0,1,\"uniform\"),\n",
    "        \"fit_prior\":[True,False],\n",
    "    }\n",
    "\n",
    "\n",
    "    # log-uniform: understand as search over p = exp(x) by varying x\n",
    "    NB_opt = BayesSearchCV(\n",
    "        MultinomialNB(),\n",
    "        search_spaces=params_NB,\n",
    "        n_iter=50,\n",
    "        cv=10,\n",
    "        n_jobs=40,\n",
    "        n_points = 4,\n",
    "        return_train_score = True\n",
    "    )\n",
    "\n",
    "\n",
    "    def on_step(optim_result):\n",
    "        score = -optim_result['fun']\n",
    "        print(\"best score: %s\" % score)\n",
    "        if score >= 0.98:\n",
    "            print('Interrupting!')\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "    NB_opt.fit(tfidf_dat[filter_q], y_dat[filter_q], callback=on_step)\n",
    "    \n",
    "    ## save model\n",
    "    with open(out+'/{}_model.p'.format(\"nb_\"+experiment+\"_tfidf_\"+str(i)), \"wb\") as f:\n",
    "        pickle.dump(NB_opt, f)\n",
    "    \n",
    "    ## save predictions\n",
    "    probs = NB_opt.predict_proba(test)\n",
    "    decision = NB_opt.predict(test)\n",
    "    with open(out+'/{}.p'.format(\"nb_\"+experiment+\"_tfidf_\"+str(i)), \"wb\") as f:\n",
    "        pickle.dump({'probs' : probs, 'decision' : decision}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    params_LOG = {\n",
    "        'C': (1e-6, 1e+6, 'log-uniform'),\n",
    "        'solver':[ \"liblinear\", \"saga\", \"lbfgs\"],\n",
    "        'max_iter':Integer(100, 500, 'uniform'),\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    # log-uniform: understand as search over p = exp(x) by varying x\n",
    "    LOGREG_opt = BayesSearchCV(\n",
    "        LogisticRegression(),\n",
    "        search_spaces=params_LOG,\n",
    "        n_iter=50,\n",
    "        cv=10,\n",
    "        n_jobs=40,\n",
    "        n_points = 4,\n",
    "        return_train_score = True\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    def on_step(optim_result):\n",
    "        score = -optim_result['fun']\n",
    "        print(\"best score: %s\" % score)\n",
    "        if score >= 0.98:\n",
    "            print('Interrupting!')\n",
    "            return True\n",
    "    \n",
    "    LOGREG_opt.fit(tfidf_dat[filter_q], y_dat[filter_q], callback=on_step)\n",
    "    \n",
    "    ## save model\n",
    "    with open(out+'/{}_model.p'.format(\"logreg_\"+experiment+\"_tfidf_\"+str(i)), \"wb\") as f:\n",
    "        pickle.dump(LOGREG_opt, f)\n",
    "    \n",
    "    \n",
    "    ## save predictions\n",
    "    probs = LOGREG_opt.predict_proba(test)\n",
    "    decision = LOGREG_opt.predict(test)\n",
    "    with open(out+'/{}.p'.format(\"logreg_\"+experiment+\"_tfidf_\"+str(i)), \"wb\") as f:\n",
    "        pickle.dump({'probs' : probs, 'decision' : decision}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    params_XG = {\n",
    "        'max_depth': Integer(1, 20, 'uniform'),\n",
    "        'learning_rate': Real(10**-5, 10**0,\"log-uniform\"),\n",
    "        \"min_samples_split\" :(2,100,\"uniform\"),\n",
    "        \"min_samples_leaf\":(2,100,\"uniform\")\n",
    "    }\n",
    "\n",
    "\n",
    "    # log-uniform: understand as search over p = exp(x) by varying x\n",
    "    XG_opt = BayesSearchCV(\n",
    "        GradientBoostingClassifier(),\n",
    "        search_spaces=params_XG,\n",
    "        n_iter=50,\n",
    "        cv=10,\n",
    "        n_jobs=40,\n",
    "        n_points = 4\n",
    "    )\n",
    "\n",
    "    def on_step(optim_result):\n",
    "        score = -optim_result['fun']\n",
    "        print(\"best score: %s\" % score)\n",
    "        if score >= 0.98:\n",
    "            print('Interrupting!')\n",
    "            return True\n",
    "\n",
    "    XG_opt.fit(tfidf_dat[filter_q], y_dat[filter_q], callback=on_step)\n",
    "    \n",
    "    ## save model\n",
    "    with open(out+'/{}_model.p'.format(\"xg_\"+experiment+\"_tfidf_\"+str(i)), \"wb\") as f:\n",
    "        pickle.dump(XG_opt, f)\n",
    "    \n",
    "    ## save predictions\n",
    "    probs = XG_opt.predict_proba(test)\n",
    "    decision = XG_opt.predict(test)\n",
    "    with open(out+'/{}.p'.format(\"xg_\"+experiment+\"_tfidf_\"+str(i)), \"wb\") as f:\n",
    "        pickle.dump({'probs' : probs, 'decision' : decision}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTTEXT EMBEDDING EVALUATION\n",
    "Now we start building models based on fasttext embeddings. We build the following models:\n",
    "1. Naive Bayes\n",
    "2. Logistic Regression\n",
    "3. Gradient Boosted Trees\n",
    "4. AutoML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "fasttext.util.download_model('de')\n",
    "ft = fasttext.load_model('cc.de.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get sentence embedding\n",
    "dat_embedding = np.array([ft.get_sentence_vector(x) for x in preprocessed])\n",
    "dat_embedding[filter_q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify test data\n",
    "test_embed = dat_embedding[-filter_q]\n",
    "test_embed_y = y_dat[-filter_q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    \n",
    "    params_NB = {\n",
    "        'var_smoothing': np.logspace(0,-9, num=100)\n",
    "    }\n",
    "\n",
    "\n",
    "    # log-uniform: understand as search over p = exp(x) by varying x\n",
    "    NB_embed_opt = BayesSearchCV(\n",
    "        GaussianNB(),\n",
    "        search_spaces=params_NB,\n",
    "        n_iter=50,\n",
    "        cv=10,\n",
    "        n_jobs=40,\n",
    "        n_points = 4\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    def on_step(optim_result):\n",
    "        score = -optim_result['fun']\n",
    "        print(\"best score: %s\" % score)\n",
    "        if score >= 0.98:\n",
    "            print('Interrupting!')\n",
    "            return True\n",
    "\n",
    "    NB_embed_opt.fit(dat_embedding[filter_q], y_dat[filter_q], callback=on_step)\n",
    "    #dump(NB_embed_opt,\"Evaluation/Baseline-Results/NB_model_embed_\"+str(i)+\"_\"+experiment)\n",
    "    \n",
    "    ## save model\n",
    "    with open(out+'/{}_model.p'.format(\"nb_\"+experiment+\"_embed_\"+str(i)), \"wb\") as f:\n",
    "        pickle.dump(NB_embed_opt, f)\n",
    "        \n",
    "    ## save predictions\n",
    "    probs = NB_embed_opt.predict_proba(test_embed)\n",
    "    decision = NB_embed_opt.predict(test_embed)\n",
    "    with open(out+'/{}.p'.format(\"nb_\"+experiment+\"_embed_\"+str(i)), \"wb\") as f:\n",
    "        pickle.dump({'probs' : probs, 'decision' : decision}, f)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    params_LOG = {\n",
    "        'C': (1e-6, 1e+6, 'log-uniform'),\n",
    "        'solver':[ \"liblinear\", \"saga\", \"lbfgs\"],\n",
    "\n",
    "    }\n",
    "\n",
    "    # log-uniform: understand as search over p = exp(x) by varying x\n",
    "    LOGREG_embed_opt = BayesSearchCV(\n",
    "        LogisticRegression(),\n",
    "        search_spaces=params_LOG,\n",
    "        n_iter=50,\n",
    "        cv=10,\n",
    "        n_jobs=20,\n",
    "        return_train_score = True\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    # callback handler\n",
    "    def on_step(optim_result):\n",
    "        score = -optim_result['fun']\n",
    "        print(\"best score: %s\" % score)\n",
    "        if score >= 0.98:\n",
    "            print('Interrupting!')\n",
    "            return True\n",
    "\n",
    "\n",
    "    LOGREG_embed_opt.fit(dat_embedding[filter_q], y_dat[filter_q], callback=on_step)\n",
    "    #dump(NB_embed_opt,\"Evaluation/Baseline-Results/NB_model_embed_\"+str(i)+\"_\"+experiment)\n",
    "    \n",
    "    ## save model\n",
    "    with open(out+'/{}_model.p'.format(\"logreg_\"+experiment+\"_embed_\"+str(i)), \"wb\") as f:\n",
    "        pickle.dump(LOGREG_embed_opt, f)\n",
    "        \n",
    "    ## save predictions\n",
    "    probs = LOGREG_embed_opt.predict_proba(test_embed)\n",
    "    decision = LOGREG_embed_opt.predict(test_embed)\n",
    "    with open(out+'/{}.p'.format(\"logreg_\"+experiment+\"_embed_\"+str(i)), \"wb\") as f:\n",
    "        pickle.dump({'probs' : probs, 'decision' : decision}, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    params_XG = {\n",
    "        'max_depth': Integer(1, 20, 'uniform'),\n",
    "        'learning_rate': Real(10**-5, 10**0,\"log-uniform\"),\n",
    "        \"min_samples_split\" :(2,100,\"uniform\"),\n",
    "        \"min_samples_leaf\":(2,100,\"uniform\"),\n",
    "    }\n",
    "\n",
    "\n",
    "    # log-uniform: understand as search over p = exp(x) by varying x\n",
    "    XG_embed_opt = BayesSearchCV(\n",
    "        GradientBoostingClassifier(),\n",
    "        search_spaces=params_XG,\n",
    "        n_iter=50,\n",
    "        cv=10,\n",
    "        n_jobs=40,\n",
    "        n_points = 4\n",
    "    )    \n",
    "\n",
    "        # callback handler\n",
    "    def on_step(optim_result):\n",
    "        score = -optim_result['fun']\n",
    "        print(\"best score: %s\" % score)\n",
    "        if score >= 0.98:\n",
    "            print('Interrupting!')\n",
    "            return True\n",
    "\n",
    "\n",
    "    XG_embed_opt.fit(dat_embedding[filter_q], y_dat[filter_q], callback=on_step)\n",
    "    #dump(NB_embed_opt,\"Evaluation/Baseline-Results/NB_model_embed_\"+str(i)+\"_\"+experiment)\n",
    "    \n",
    "    ## save model\n",
    "    with open(out+'/{}_model.p'.format(\"xg_\"+experiment+\"_embed_\"+str(i)), \"wb\") as f:\n",
    "        pickle.dump(XG_embed_opt, f)\n",
    "        \n",
    "    ## save predictions\n",
    "    probs = XG_embed_opt.predict_proba(test_embed)\n",
    "    decision = XG_embed_opt.predict(test_embed)\n",
    "    with open(out+'/{}.p'.format(\"xg_\"+experiment+\"_embed_\"+str(i)), \"wb\") as f:\n",
    "        pickle.dump({'probs' : probs, 'decision' : decision}, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
